# -*- coding: utf-8 -*-
"""BDMH_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tdqYCC33Z3UdPI_ppsqb780R-lWy7qdk
"""

#@title Imports
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2, f_classif, mutual_info_classif
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.neural_network import MLPClassifier as MLC
from sklearn import tree
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2, f_classif, mutual_info_classif
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import LinearSVC

data = pd.read_csv("processed.cleveland.data", header = None)  #Read Cleveland.data in data frame
data2 = pd.read_csv("heart.csv", header = None) #read heart.csv in dataframe
#data.isnull().sum()
data2.isnull().sum() #Check if there is any null values.

all_data = data.values
totalNan=0
for i in range(all_data.shape[0]):
  for j in range(all_data.shape[1]):
    if all_data[i][j] == '?' :  #check if any null values. If value is '?' then replace with NAN. 
      all_data[i][j] = np.nan
      totalNan+=1
    j += 1
  i += 1

print(totalNan)
all_data=all_data.astype('float')

all_data=pd.DataFrame(all_data)
print(all_data.isnull().sum()) #This check number of NAN Values in the data.

for column in all_data.columns:
  all_data[column].fillna(all_data[column].mean(),inplace=True)  #Replace NAN value with mean value of respective column

print(all_data.isnull().sum()) #Check if all NAN values are replaced.

all_data = all_data.sample(frac=1).reset_index(drop=True)  #Shuffle all dataset.

all_data=np.array(all_data)
# all_data = all_data[~np.isnan(all_data).any(axis=1)]
labels = all_data[:, -1]  # Extract labels.
features = all_data[:, :-1]  #Extract Features.

for i in range(len(labels)):
  if labels[i] != 0 :
    labels[i] = 1.0
  i += 1

heart = data2.values
heart_labels = heart[:, -1]
for i in range(len(heart_labels)):
  if heart_labels[i] == 1.0 :
    heart_labels[i] = 0.0  #Convert second file in first dile format.
  elif heart_labels[i] == 2.0:
    heart_labels[i] = 1.0
  i += 1

heart_features = heart[:, :-1]
heart_features.shape
heart_labels.shape

heart_labels = np.concatenate([labels,heart_labels])
heart_features = np.concatenate([features,heart_features])  #Concatenate first and second file
heart_labels.shape
heart_features.shape

one=0
zero=0
for labels in heart_labels:
  if labels==1:
    one+=1
  else:
    zero+=1
print(one,zero)   #Check if there is any class imbalance.

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(heart_features) #scale the data so that none of the attribute dominate.
scaler.transform(heart_features)

#X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(heart_features, heart_labels, test_size=0.15)

from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt


def plotGraph(y_test,y_pred): #function to plot PR curve, ROC curve and Fscore.
  
  print(confusion_matrix(y_test, y_pred,labels=[0,1]))  #prints Confusion matrix tn,fp,fn,tp
  #tn,fp,fn,tp=confusion_matrix(y_test, y_pred,labels=[0,1])
  #print(tn,fp,fn,tp)
  precision=[]
  recall=[]
  fscore=[]
  tp=0
  fp=0
  x_axis=[]
  FPR=[]
  for j in range(len(y_pred)):
    if y_test[j]==y_pred[j] and y_test[j]==1:
      tp+=1   #True positive for calculating PRECISION and RECALL

    if y_test[j]==0 and y_pred[j]==1:
      fp+=1 # False Positive for calculating False Positive Rate

    pre=tp/(j+1)
    precision.append(pre)   #Append Precision at each point.

    rec=tp/(list(y_test).count(1))     #Append Recall at each point.
    recall.append(rec)

    fprsc=fp/(list(y_test).count(0))   #Append FPR at each point.
    FPR.append(fprsc)

    fs=(2*pre*rec)/(pre+rec+1)   #Append Fscore at each point.
    fscore.append(fs)

    x_axis.append(j)


  plt.plot(recall,precision)    #Plots Precision Recall Curve
  plt.title("PR Curve")
  plt.xlabel("Recall")
  plt.ylabel("Precision")
  plt.show()

  plt.plot(x_axis,fscore) #Plots Fscore Curve
  plt.title("F-Score")
  plt.xlabel("Points")
  plt.ylabel("F-Score")
  plt.show()


  plt.plot(FPR,recall)  #Plots ROC Curve
  plt.title("ROC")
  plt.xlabel("FPR")
  plt.ylabel("Recall")
  plt.show()

from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

i=0
dictModels={}
dictModels["MLP"]=0
dictModels["Decision Tree"]=0
dictModels["Naive Bayes"]=0
dictModels["Logistic Regression"]=0
dictModels["SVM"]=0
dictModels["Random Forest"]=0
dictModels["KNN"]=0

X_train_orig=heart_features
y_train_orig=heart_labels
skf = StratifiedKFold(n_splits=5)

y_test_max=[]
y_pred_max=[]

for i in range(0,7):
  for train_index, test_index in skf.split(X_train_orig, y_train_orig):
      #print(test_index,train_index)
      #This runs For 5 times i.e 5 folds. For Each model We consider maximun Accuracy from 5 folds.
      X_train, X_test = X_train_orig[train_index], X_train_orig[test_index]
      y_train, y_test = y_train_orig[train_index], y_train_orig[test_index]

      pca = PCA(n_components=10).fit(X_train)
      X_train = pca.transform(X_train)
      X_test = pca.transform(X_test)
      #print(X_train.shape,X_test.shape)
      if i==0:
        clf = MLPClassifier(alpha=0.0001, max_iter=1000, learning_rate='adaptive', activation='relu') #MLP Classifier
        clf.fit(X_train, y_train)
        acc=accuracy_score(y_test, clf.predict(X_test))
        if acc>dictModels["MLP"]:  #Checks Whether gained accuracy is maximum or not. 
          dictModels["MLP"]=acc
          y_test_max=[]
          y_pred_max=[]
          y_test_max=y_test
          y_pred_max=clf.predict(X_test)

      elif i==1:
        clf = DecisionTreeClassifier(max_leaf_nodes=10000) # Decision Tree Classifier
        clf.fit(X_train, y_train)
        acc=accuracy_score(y_test, clf.predict(X_test))
        if acc>dictModels["Decision Tree"]:
          dictModels["Decision Tree"]=acc
          y_test_max=[]
          y_pred_max=[]
          y_test_max=y_test
          y_pred_max=clf.predict(X_test)
          
      elif i==2:
        clf = GaussianNB()  #Naive BAyes Classifier.
        clf.fit(X_train, y_train)
        acc=accuracy_score(y_test, clf.predict(X_test))
        if acc>dictModels["Naive Bayes"]:
          dictModels["Naive Bayes"]=acc
          y_test_max=[]
          y_pred_max=[]
          y_test_max=y_test
          y_pred_max=clf.predict(X_test)
          
      elif i==3:
        clf = LogisticRegression()  #Logistic Regression classifier.
        clf.fit(X_train, y_train)
        acc=accuracy_score(y_test, clf.predict(X_test))
        if acc>dictModels["Logistic Regression"]:
          dictModels["Logistic Regression"]=acc
          y_test_max=[]
          y_pred_max=[]
          y_test_max=y_test
          y_pred_max=clf.predict(X_test)
          
      elif i==4:
        clf = svm.SVC(kernel="poly")  #SVM Classifier.
        clf.fit(X_train, y_train)
        acc=accuracy_score(y_test, clf.predict(X_test))
        if acc>dictModels["SVM"]:
          dictModels["SVM"]=acc
          y_test_max=[]
          y_pred_max=[]
          y_test_max=y_test
          y_pred_max=clf.predict(X_test)
          
      elif i==5:
        clf = RandomForestClassifier(max_depth=10, random_state=0)  #Random Forest Classifier.
        clf.fit(X_train, y_train)
        acc=accuracy_score(y_test, clf.predict(X_test))
        if acc>dictModels["Random Forest"]:
          dictModels["Random Forest"]=acc
          y_test_max=[]
          y_pred_max=[]
          y_test_max=y_test
          y_pred_max=clf.predict(X_test)
          
      elif i==6:
        clf = KNeighborsClassifier(n_neighbors=5)  #KNN Classifier.
        clf.fit(X_train, y_train)
        acc=accuracy_score(y_test, clf.predict(X_test))
        if acc>dictModels["KNN"]:
          dictModels["KNN"]=acc
          y_test_max=[]
          y_pred_max=[]
          y_test_max=y_test
          y_pred_max=clf.predict(X_test)

  if i==0:
    print("MLP Classifier")
  elif i==1:
    print("Decision Tree")
  elif i==2:
    print("Naive Bayes")
  elif i==3:
    print("Logistic Regression")
  elif i==4:
    print("SVM")
  elif i==5:
    print("Random Forest")
  else:
    print("KNN")

  plotGraph(y_test_max,y_pred_max)  #pass y_test and y_pred of the fold which has highest accuracy score.
print(dictModels)

from keras.models import Sequential
from keras.layers import LSTM, Dense

#Use LSTM model to predict values.

X_train, X_test, y_train, y_test = train_test_split(heart_features, heart_labels, test_size=0.15)

model = Sequential()
model.add(LSTM(128,return_sequences=False,input_shape=(1,13)))  


model.add(Dense(2, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])

train=np.reshape(X_train, (X_train.shape[0],1,X_train.shape[1]))

test=np.reshape(X_test, (X_test.shape[0],1,X_test.shape[1]))

seq_length = 64

model.fit(train, np.array(y_train),batch_size=5, epochs=51)

y_pred=model.predict_classes(test,verbose=0)

print(accuracy_score(y_test, y_pred ))

print("LSTM")
plotGraph(y_test,y_pred)

dictModels["LSTM"]=accuracy_score(y_test,y_pred)
print(dictModels)

#Model Accuracy Score Comparison Graph.
import matplotlib.pyplot as plt
plt.plot(list(dictModels.keys()),list(dictModels.values()),color='green', linestyle='dashed', linewidth = 3, 
         marker='o', markerfacecolor='blue', markersize=12)
plt.xlabel("Models")
plt.ylabel("Score")
plt.title("Accuracy Score Comparison")
plt.xticks(rotation=90)
plt.show()